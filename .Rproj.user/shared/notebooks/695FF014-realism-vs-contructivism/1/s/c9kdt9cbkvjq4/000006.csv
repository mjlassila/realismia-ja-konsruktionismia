"0","project_dir <- here::here()"
"0","stopwords <- base::readLines(paste0(project_dir,'/data/stopwords.txt'))"
"2","incomplete final line found on '/Users/matti/Documents/2018/realism-vs-constructivism/data/stopwords.txt'"
"0","constructivist_dataset <- readr::read_csv2(paste0(project_dir,'/processed_data/constructivist-dataset.csv'))"
"2","Using ',' as decimal and '.' as grouping mark. Use read_delim() for more control.
"
"2","Parsed with column specification:
cols(
  epistemology = col_character(),
  document = col_character()
)
"
"0","realist_dataset <- readr::read_csv2(paste0(project_dir,'/processed_data/critical-realist-dataset.csv'))"
"2","Using ',' as decimal and '.' as grouping mark. Use read_delim() for more control.
"
"2","Parsed with column specification:
cols(
  epistemology = col_character(),
  document = col_character()
)
"
"0","combined_data <- base::rbind(constructivist_dataset, realist_dataset)"
"0","constructivist_documents <- readLines(paste0(project_dir,'/processed_data/constructivist-documents.txt'))"
"0","realist_documents <- readLines(paste0(project_dir,'/processed_data/critical-realist-documents.txt'))"
"0","combined_documents <- c(constructivist_documents, realist_documents)"
"0","processed <- stm::textProcessor("
"0","  combined_data$document,"
"0","  metadata = combined_data,"
"0","  stem = FALSE,"
"0","  customstopwords = stopwords)"
"1","Building corpus... 
"
"1","Converting to Lower Case... 
"
"1","Removing punctuation... 
"
"1","Removing stopwords... 
"
"1","Remove Custom Stopwords...
"
"1","Removing numbers... 
"
"1","Creating Output... 
"
"0","out <- stm::prepDocuments("
"0","  processed$documents,"
"0","  processed$vocab,"
"0","  processed$meta,"
"0","  lower.thresh = 3)"
"1","Removing 22845 of 28486 terms (29363 of 103444 tokens) due to frequency 
"
"1","Your corpus now has 44 documents, 5641 terms and 74081 tokens."
